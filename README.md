
# CADRNet

**CADRNet: Geometric Figure Recognition in Architectural Drawings via Dynamic Feature Pyramid and Cross-Architecture Adapter**


---

## ğŸ” Introduction

CADRNet is a hybrid deep learning architecture designed to recognize geometric symbols in architectural CAD drawings. It tackles challenges such as:

- Large scale variations among symbols  
- Complex topological dependencies  
- Noisy or blurred drawing content  

CADRNet achieves this through:

- **DFPN (Dynamic Feature Pyramid Network)**: Attention-guided, multi-scale feature extraction  
- **CTAdapter (Cross-Architecture Adapter)**: Bidirectional fusion between CNN and Transformer branches  

---

## ğŸ§  Core Contributions

1. **CNN-Transformer Hybrid Backbone**  
   Combines local texture perception (CNN) with global semantic modeling (Transformer)

2. **DFPN**  
   Adaptive fusion of shallow and deep feature maps for scale-robustness

3. **CTAdapter**  
   Cross-architecture, context-aware fusion with axial pooling and residual gating

4. **Training Strategy**  
   Joint supervision with focal loss and BCE loss, tailored for class imbalance and boundary sharpness

---

## ğŸ“‚ Dataset: FloorPlanCAD

We use the **FloorPlanCAD** dataset introduced by [Fan et al., ICCV 2021](https://doi.org/10.1109/ICCV48922.2021.00997), a large-scale benchmark for panoptic symbol recognition in architectural CAD drawings.

### ğŸ”§ Key Features:
- **15,000+ floor plans**: residential and commercial buildings
- **Rich annotation**: walls, windows, HVAC, pipelines, etc.
- **Symbol scale**: from **<32 px to >512 px**
- **Topology-aware**: nested rooms, pipe chains, etc.
- **Challenges**: long-tail distribution, noise, fuzzy boundaries

### ğŸ“ Preprocessed Directory Structure

```bash
datasets/
â””â”€â”€ floorplancad_v2/
    â”œâ”€â”€ images/         # Rasterized PNGs
    â”œâ”€â”€ labels/         # Ground truth masks
    â”œâ”€â”€ svg/            # Original vector drawings
    â”œâ”€â”€ npy/            # Numpy-processed data
```

### âš™ï¸ Preprocessing Commands

```bash
# Convert SVG to PNG
python preprocess/svg2png.py --train_00 ./datasets/svg_raw/train --svg_dir ./datasets/svg_processed/svg --png_dir ./datasets/svg_processed/png --scale 7 --cvt_color

# Generate .npy from vectorized CAD
python preprocess/preprocess_svg.py -i ./datasets/svg_processed/svg/train -o ./datasets/svg_processed/npy/train --thread_num 64

# Create symbolic link
mkdir -p ./data
ln -s /your/path/to/datasets/svg_processed ./data/floorplancad_v2
```

---

## ğŸ› ï¸ Environment Setup

```bash
# Install Anaconda (Linux CentOS 8.0)
wget https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Linux-x86_64.sh
chmod +x Anaconda3-2024.10-1-Linux-x86_64.sh
./Anaconda3-2024.10-1-Linux-x86_64.sh

# Create conda environment
conda create -n CADRNet python=3.7.7 -y
conda activate CADRNet

# Install dependencies
conda install pytorch=1.9.0 torchvision=0.10.0 cudatoolkit=11.1 -c pytorch -c nvidia -y
conda install scikit-learn=1.0.1 pillow=8.3.1 matplotlib scipy tqdm git -y
pip install opencv-python gdown svgpathtools timm==0.4.12
```

---

## ğŸš€ Training & Testing

### ğŸ”§ Multi-GPU Distributed Training (DDP)

```bash
# 2 GPU (e.g. NVIDIA A10x2x24GB)
CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 train_cad_ddp.py --data_root ./data/floorplancad_v2 --pretrained_model ./pretrained_models/hrnetv2_w48_imagenet_pretrained.pth
```

### ğŸ“ˆ Evaluation on Test Set

```bash
# Load best model and run test
python -m torch.distributed.launch --nproc_per_node=2 train_cad_ddp.py --data_root ./data/floorplancad_v2 --pretrained_model ./pretrained_models/hrnetv2_w48_imagenet_pretrained.pth --test_only --load_ckpt ./logs/best_model.pth
```

---

## ğŸ“Š Evaluation Metrics

We adopt the **panoptic segmentation evaluation protocol** proposed by [Kirillov et al., 2019], which includes:

| Metric | Description |
|--------|-------------|
| **PQ (Panoptic Quality)** = RQ Ã— SQ | Overall metric for joint segmentation and recognition |
| **SQ (Segmentation Quality)** | Measures IoU overlap between predicted and ground-truth masks |
| **RQ (Recognition Quality)** | Measures detection/recognition precision and recall |

```bash
# Compute PQ, SQ, and RQ
python scripts/evaluate_pq.py --raw_pred_dir ./data/output/raw_predictions/ --svg_pred_dir ./data/svg_predictions/ --svg_gt_dir ./data/svg_ground_truth/ --thread_num 64
```

---

## ğŸ“ Project Structure

```
CADRNet/
â”œâ”€â”€ model/              # CADRNet architecture (DFPN, CTAdapter, etc.)
â”œâ”€â”€ dataset/            # Data loaders and preprocessing
â”œâ”€â”€ train_cad_ddp.py    # Main training script with DDP
â”œâ”€â”€ preprocess/         # Data conversion (SVG to PNG/Numpy)
â”œâ”€â”€ scripts/            # Evaluation utilities
â”œâ”€â”€ config.py           # Hyperparameter settings
â””â”€â”€ README.md
```

---

## ğŸ“š Citation

**CADRNet: Geometric Figure Recognition in Architectural Drawings via Dynamic Feature Pyramid and Cross-Architecture Adapter**

---

## ğŸ“¬ Contact

For any questions or collaborations:  
ğŸ“§ hi-bruce.yin@connect.polyu.hk
